



研究背景：

階層的強化学習 (HRL) は、探索において有益であると知られており \cite{19}、効率的な報酬割り当て、転移学習、解釈性などといった点で優位性があると考えられている\cite{15}。

また、世界モデルは高いサンプル効率を実現し、エージェントが少ない環境との相互作用で学習することを可能にする。

この２つの要素を組み合わせたモデルに、Director \cite{}、Hierosがあり、ベースラインとして使用されている世界モデルを上回る性能を示している。
一方で、解釈性やモデルをスケールさせる方向での研究は知られておらず、階層的な世界モデルの理論的および実証的なは未だ未開拓であると言える。


関連研究



同様に、Heatmapについても、報酬割当実験、エントロピー調整実験、報酬設計実験の3つについて、一つのプロットでの100k, 200k, 300k, 400kではなく、すべてのプロットでの400kを表示してほしい。


深層強化学習 (Deep RL) は、ゲームやロボット制御などの多様なタスクにおいて超人的な性能を達成してきた。特に、環境のダイナミクスを学習し、その内部モデルを用いて予測・計画を行う「世界モデル (World Models)」は、高いサンプル効率と汎化性能を実現するアプローチとして注目されている \cite{3, 5}。DreamerV3 \cite{5} に代表される世界モデルは、画像の再構成と報酬予測を通じて潜在空間を獲得し、想像上のロールアウトを用いて方策を学習することで、Atari 100k ベンチマーク等で著しい成果を上げている。

しかし、標準的な世界モデルは、長期的な計画が求められるタスクや報酬が極端に疎な環境において、予測誤差の蓄積や勾配消失などの課題がある。
この課題に対し、階層的世界モデル (Hierarchical World Models, HWM) が提案されている。
代表的な手法である Director \cite{1} や Hieros \cite{2} は、上位層が下位層に対して抽象的なサブゴール（目標状態）を提示し、下位層がそれを達成するように動作することで、長期的なタスク分解を可能にするとされている。
特に Hieros は、S5WM を導入することで計算効率と長期記憶能力を向上させ、Atari において従来手法を上回るスコアを記録した。

一方、これらの階層的世界モデルは、本当に「長期的な計画」や「階層的な推論」を獲得しているのだろうか？ あるいは、単にモデル容量の増大や探索のランダム性が、Atari のような反射的なタスクにおいて有利に働いただけなのだろうか？ 
既存研究の多くはベンチマークのスコア向上に主眼を置いており、モデルが実際にどのような階層的構造を学習しているか、あるいは推論能力が真に向上しているかについての検証は不十分である。

本研究では、現在の階層的世界モデルの到達点と限界を明らかにするために、Hieros を対象とした詳細な検証を行う。Atari ベンチマークに加え、論理的な順序推論と長期記憶が不可欠な「Visual Pinpad」環境を用いて評価を行った。実験の結果、Hieros は Director (RSSMベースの階層モデル) が解決可能な Pinpad タスクにおいて、報酬設計を工夫しても学習が全く進まないという重大な課題が明らかになった。さらに、Atari 環境における内部状態と方策の可視化を行ったところ、高スコアを獲得しているタスクであっても、実際には「常に前進し続ける」といった極めて単純（縮退した）方策に収束している事例が確認された。

本稿の貢献は以下の通りである。
\begin{itemize}
    \item 階層的世界モデル Hieros の詳細な再評価を行い、長期推論タスクにおける脆弱性を実証的に示した。
    \item Atari における高スコアが必ずしも高度な計画能力を意味しないことを、方策の可視化を通じて指摘した。
    \item S5WM を用いた階層化における学習の不安定性や、タスクによる向き不向きを分析し、今後の階層的世界モデル研究における新たな課題（ベンチマークの選定や評価指標の妥当性）を提言する。
\end{itemize}

これらの知見は、単にスコアを追い求めるのではなく、真に長期的な推論が可能なエージェントを構築するための重要な示唆を与えるものである。
