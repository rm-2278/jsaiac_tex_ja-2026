\usepackage{color}
\usepackage[dvipdfmx]{graphicx}
\usepackage{url}
%%
\title{
\jtitle{階層的世界モデルにおける現状と課題：Hierosの限界と将来の展望}
\etitle{Current Status and Challenges in Hierarchical World Models: Identifying Limitations of Hieros and Future Prospects}
}
\jaddress{氏名，所属，住所，rm2278@cam.ac.uk}

\author{%
\jname{三好 理輝\first}
\ename{Riki Miyoshi}
\and
\jname{劉 智優\second}
\ename{Jiu You}
\and
\jname{山田 達也\third}
\ename{Yamada Tatsuya}
}

\affiliate{
\jname{\first{}ケンブリッジ大学}
\ename{University of Cambridge}
\and
\jname{\second{}所属}
\ename{Affiliation \#2 in English}
\and
\jname{\third{}大阪大学}
\ename{The University of Osaka}
}

%%
\begin{abstract}
   ここに\\
   アブストラクトを\\
   5行程度\\
   書き\\
   ます\\
\end{abstract}

%\setcounter{page}{1}
\def\Style{``jsaiac.sty''}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em%
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\JBibTeX{\leavevmode\lower .6ex\hbox{J}\kern-0.15em\BibTeX}
\def\LaTeXe{\LaTeX\kern.15em2$_{\textstyle\varepsilon}$}

\begin{document}
\maketitle

\section{はじめに}

\section{研究背景・目的}
階層的強化学習 (HRL) は、探索において有益であると知られており \cite{9}、効率的な報酬割り当て、転移学習、解釈性などといった点で優位性があると考えられている\cite{14}。

また、世界モデルは高いサンプル効率を実現し、エージェントが少ない環境との相互作用で学習することを可能にする。

この２つの要素を組み合わせたモデルに、Director\cite{1}、Hieros\cite{2}があり、ベースラインとして使用されている世界モデルを上回る性能を示している。
一方で、解釈性やモデルをスケールさせる方向での研究は知られておらず、階層的な世界モデルの理論および評価実験は知られていない。

そこで本研究では、階層的世界モデルの現状を理解し発展方向を特定するために、Hierosの性能評価および内部状態の理解を試みた。


\section{関連研究}
\subsection{世界モデル}
世界モデルは、「説明」。
世界モデルは大きく、1. 状態空間モデル、2. 状態予測モデル、3. 観測予測モデル に分けることができる。
代表的な世界モデルとして、Dreamer および TD-MPC2が挙げられる。
Dreamerは、状態空間モデルの一つであり、

\subsection{階層的強化学習 (HRL)}



\subsection{階層的世界モデル (HWM)}
階層的世界モデルも、世界モデルと同様、
状態空間モデルであるDreamerの方策を階層化したものがDirectorである。マネージャーとワーカーの２つの方策が存在し、マネージャーはより長い時間軸で目標とする状態を圧縮された形でワーカーに渡し、ワーカーは通常の強化学習と同様の短い時間軸で行動を出力する。
ワーカーの損失関数にマネージャーの提示した目標がどの程度達成されたかを組み込むことで、ワーカーとマネージャーの２つの方策がそれぞれ役割分担しながら同時に学習できるようにしている。

Directorの拡張としてHierosがある。
Hierosでは、
\begin{enumerate}
   \item 方策だけでなく世界モデルも階層化している。
   \item 2階層だけでなくそれより多い階層でも実験を行っている。
   \item 計算効率を上げるために、状態空間モデルとしてRSSMではなくS5WMを用いている。
   \item リプレイバッファからのサンプリングを工夫し、偏りを減らすことと時間計算量の削減を実現している。
\end{enumerate}
という工夫が加えられたモデルが提案されており、AtariにおいてDreamerV3を超える性能を示した。
本稿では方策だけでなく世界モデルも階層化しているという点でHierosに着目し、その限界と可能性を調べる。


THICKも、Directorをベースとして提案された手法である。「説明」。


状態予測モデルであるTD-MPC2を階層的にしたものがPuppeteerである。これは、「説明」




\section{実験・考察}
本稿では、階層的世界モデルのHieros\cite{2}モデルの評価を行った。
特に、論文中では検証されていなかった、長期タスクの評価に適しているVisual Pinpad環境での検証を行った。また、Atariでのモデルの内部状態の確認を行った。

\subsection{手法}
Hierosのモデル検証では、既存のレポジトリ\cite{11}を変更して実験を行った。
また、Directorではこのレポジトリ\cite{12}を用いて実験を行った。
RTX-5070TiまたはRTX-6000一つを用いて学習をした。

PinpadにおけるHierosの学習状況を調査した。まずは、Hierosのベースラインでの検証を行った。
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/Hieros-baseline.png}
  \caption{Visual Pinpad環境におけるHierosの学習および内部報酬曲線。}
  \label{fig:hieros-baseline}
\end{figure}

図からわかるように、報酬が0のまま学習が進まないことが確認された。
そこで、以下ではpinpadの報酬設計を、から、


より詳しくHierosの性能について調査するために、Hierosの各レイヤが提示している状態の可視化を行った。元の論文では過去の経験を参考にしたノイズを加えての可視化を行っていたが、モデルの予測のみに焦点を当てるために、ノイズなしでの可視化を行った。
update-everyは8で実験を行った。\subsection{時間的抽象化の調整}

update-everyは、それぞれの階層が、下の階層と比較してどの程度時間的抽象化を行っているかを表している。例えば、update-every=4では、4ステップごとに上の階層が下の階層に与えるサブゴールを更新しているという意味である。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-episode-scores.png}
  \caption{異なるsubactor-update-everyパラメータでの学習曲線の比較。}
  \label{fig:sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-subgoal-temporal-compressed.png}
  \caption{学習過程におけるサブゴール可視化の時間的変化（上位2層）。}
  \label{fig:subgoal-temporal}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-heatmap-temporal.png}
  \caption{探索位置のヒートマップの時間的変化。}
  \label{fig:heatmap-temporal}
\end{figure}

図からわかるように、多少の差異がある一方で、最終的に報酬が1になる程度に方策が収束している。

\subsection{報酬割り当て係数の変更}
Hierosでは、各レイヤごとの方策は、external reward, subgoal rewardとintrinsic rewardの3つを用いて学習を行っている。
これらの比率を変更することによって、学習がどう変化するかを確認した。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-ratio-sweep/sweep-episode-scores.png}
  \caption{異なる報酬割り当て係数での学習曲線の比較（novelty\_reward\_weightごとに分割）。}
  \label{fig:reward-ratio-sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-ratio-sweep/sweep-subgoal-temporal-compressed.png}
  \caption{報酬割り当て実験におけるサブゴール可視化の時間的変化（上位2層）。}
  \label{fig:reward-ratio-subgoal-temporal}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-ratio-sweep/sweep-heatmap-temporal.png}
  \caption{報酬割り当て実験における探索位置のヒートマップの時間的変化。}
  \label{fig:reward-ratio-heatmap-temporal}
\end{figure}

限定的な向上が確認でき、報酬割り当て係数を変更することの有効性が示唆される。

\subsection{方策エントロピーの変更}

方策エントロピーを変更したときの結果を確認した。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/entropy-sweep/sweep-episode-scores.png}
  \caption{異なるactor\_entropyパラメータでの学習曲線の比較。}
  \label{fig:entropy-sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/entropy-sweep/sweep-subgoal-temporal-compressed.png}
  \caption{エントロピー調整実験におけるサブゴール可視化の時間的変化（上位2層）。}
  \label{fig:entropy-subgoal-temporal}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/entropy-sweep/sweep-heatmap-temporal.png}
  \caption{エントロピー調整実験における探索位置のヒートマップの時間的変化。}
  \label{fig:entropy-heatmap-temporal}
\end{figure}

エントロピーが大きくなるとより収束しにくくなり報酬も高くなったが、顕著に学習が進むことはなかった。

\subsection{Pinpadの報酬設計の変更}

pinpad-easy-threeタスクにおいて、報酬設計を変更した実験結果。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-design-sweep/sweep-episode-scores.png}
  \caption{Pinpad-easy\_threeタスクにおける異なる報酬設計での学習曲線の比較。}
  \label{fig:reward-design-sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-design-sweep/sweep-subgoal-temporal-compressed.png}
  \caption{報酬設計実験におけるサブゴール可視化の時間的変化（上位2層）。}
  \label{fig:reward-design-subgoal-temporal}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-design-sweep/sweep-heatmap-temporal.png}
  \caption{報酬設計実験における探索位置のヒートマップの時間的変化。}
  \label{fig:reward-design-heatmap-temporal}
\end{figure}

\subsection{考察}
実験当初はpinpadで学習が進まない理由が報酬が疎であるからだと考えていたが、Pinpadの報酬設計を密にした時にも学習が進まなかったため、そうでは無いと考えた。
さらにAtariのモデルの可視化もする中で、Atariでも有意義な方策が学習できていないと思われ、Hierosのモデルそのものに、不安定性あるいは改良の余地があるのではないかと考える。
一方で、有意義だと思えない学習をしているにも関わらず、なぜHierosがAtariで性能が高く出ていたのかの理由の解明も今後の課題である。
Atariの最適方策が学習しやすく、\cite{9}で提示されているような階層化による探索の改善が関連しているとも考えられるが、検証が必要である。
検証結果からはHierosの上手く学習ができておらず汎化性能が低く、より頑健性の高いモデルの理論的、実験的模索が必要であると結論づける。


ベースラインコードにエラーが含まれている可能性は否めない一方で、世界モデルを階層的にすることによって学習が不安定になっている可能性も高い。
特に世界モデルは目的関数や誤差蓄積に起因して、また階層的モデルは階層の同時学習による非定常性によってハイパーパラメータに繊細であることが知られており、それらの組み合わせによって学習が不安定になってしまっている可能性もある。
この理論的、実証実験的解析は今後の課題である。


\section{まとめ}





\section{展望}
本稿ではHierosの3つの報酬係数を手動で設定し比較を行ったが、HarmonyDream \cite{13} のように、報酬を自動でバランスすると、より多様なタスクにおいて学習ができるようになると予想する。

\section{Appendix}
\subsection{Atari}
HierosのAtariの実験を再現し、モデルの学習を確認した。スコアは確かに論文通り高かった一方で、policy-imageを確認すると単純な方策が学習されており（freewayではただ前に進む）、Hierosでは複雑な方策を学べていないのではないかと考える。
なお計算資源の都合上、freewayのみはHierosの元の論文と同じで(batch size, batch length)が(16, 64)、他は(16, 16)で検証を行った。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_freeway-scores.png}
  \caption{Atari Freewayタスクの学習曲線（seedで平均）。}
  \label{fig:atari-freeway-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_freeway-policy-temporal.png}
  \caption{Atari Freewayタスクにおける方策の時間的変化。}
  \label{fig:atari-freeway-policy}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_breakout-scores.png}
  \caption{Atari Breakoutタスクの学習曲線（seedで平均）。}
  \label{fig:atari-breakout-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_breakout-policy-temporal.png}
  \caption{Atari Breakoutタスクにおける方策の時間的変化。}
  \label{fig:atari-breakout-policy}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_krull-scores.png}
  \caption{Atari Krullタスクの学習曲線（seedで平均）。}
  \label{fig:atari-krull-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_krull-policy-temporal.png}
  \caption{Atari Krullタスクにおける方策の時間的変化。}
  \label{fig:atari-krull-policy}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_battle_zone-scores.png}
  \caption{Atari Battle Zoneタスクの学習曲線（seedで平均）。}
  \label{fig:atari-battlezone-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/atari/atari_battle_zone-policy-temporal.png}
  \caption{Atari Battle Zoneタスクにおける方策の時間的変化。}
  \label{fig:atari-battlezone-policy}
\end{figure}


\subsection{Director}
Directorを用いたPinpad-3およびPinpad-easy-3の結果。学習が進んでいることが確認される。pinpad-3の結果は\cite{1}で報告されている結果に近いものとなっている。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/director-results/director-episode-scores.png}
  \caption{DirectorによるPinpad-3およびPinpad-dense-3の学習曲線。}
  \label{fig:director-scores}
\end{figure}

また、比較のために、作成したpinpad-denseでの学習も行い、高い性能が出ることを確認した。これにより、報酬設計には問題がないことが確認された。
なお、GPUのメモリの都合上途中までしか学習ができなかった。

\subsection{RSSM}
HierosはS5WMを用いているが、ベースラインにはRSSMも用意されている。以下は、RSSMを用いてHierosをpinpadで結果。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/rssm-sweep/sweep-episode-scores.png}
  \caption{RSSMを用いたHierosのPinpadでの学習曲線。}
  \label{fig:rssm-scores}
\end{figure}

学習が進まなかったため、max-hierarchy=1のときは320kで学習を打ち切った。
階層数を少ない時により学習が進んでいることから、階層を加えることがモデルの頑健性を下げていることが推測できる。

\begin{thebibliography}{99}
\bibitem[Director]{1} Hafner, D. et al.: Deep Hierarchical Planning from Pixels, NeurIPS (2022).
\bibitem[Hieros]{2} Mattes, et al.: Hieros - Hierarchical Imagination on Structured State Space Sequence World Models (2023).
\bibitem[DreamerV1]{3}
\bibitem[DreamerV2]{4}
\bibitem[DreamerV3]{5}
\bibitem[TD-MPC]{6}
\bibitem[TD-MPC2]{7} Hansen, N. et al.: TD-MPC2: Scalable, Robust World Models for Continuous Control, ICLR (2024).
\bibitem[Puppeteer]{8} Hansen, N. et al.: Hierarchical World Models as Visual Whole-Body Humanoid Controllers, ICLR (2025).
\bibitem[HRL benefit]{9} Nachum, O. et al.: Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?, NeurIPS (2019).
\bibitem[THICK]{10} Gumbsch, C. et al.: Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics, EWRL (2023).
\bibitem[Hieros Repo]{11} \url{https://github.com/Snagnar/Hieros}
\bibitem[Director Repo]{12} \url{https://github.com/danijar/director}
\bibitem[HarmonyDream]{13} Ma, H. et al.: HarmonyDream: Task Harmonization Inside World Models, ICML (2024).
\bibitem[HRL survey]{14} Pateria, S. et al.: Hierarchical Reinforcement Learning: A Comprehensive Survey, ACM Computing Surveys (2021). 
\end{thebibliography}

