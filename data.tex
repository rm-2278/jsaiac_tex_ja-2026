\usepackage{color}
\usepackage[dvipdfmx]{graphicx}
\usepackage{subcaption}
\usepackage{url}
\usepackage{amsmath}
%%
\title{
\jtitle{階層的世界モデルの現状と課題：Hierosの限界と将来の展望}
\etitle{Current Status and Challenges in Hierarchical World Models: Identifying Limitations of Hieros and Future Prospects}
}
\jaddress{三好理輝，rm2278@cam.ac.uk}

\author{%
\jname{三好 理輝\first}
\ename{Riki Miyoshi}
\and
\jname{劉 智優\second}
\ename{Jiu You}
\and
\jname{山田 達也\third}
\ename{Yamada Tatsuya}
}

\affiliate{
\jname{\first{}ケンブリッジ大学}
\ename{University of Cambridge}
\and
\jname{\second{}電気通信大学}
\ename{University of Electro-Communications}
\and
\jname{\third{}大阪大学}
\ename{The University of Osaka}
}

%%
\begin{abstract}
   ここに\\
   アブストラクトを\\
   5行程度\\
   書き\\
   ます\\
\end{abstract}

%\setcounter{page}{1}
\def\Style{``jsaiac.sty''}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em%
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\JBibTeX{\leavevmode\lower .6ex\hbox{J}\kern-0.15em\BibTeX}
\def\LaTeXe{\LaTeX\kern.15em2$_{\textstyle\varepsilon}$}

\begin{document}
\maketitle

\section{はじめに}
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\
\section{研究背景・目的}
階層的強化学習 (HRL) は、探索において有益であると知られており \cite{9}、効率的な報酬割り当て、転移学習、解釈性などといった点で優位性があると考えられている\cite{14}。

また、世界モデルは高いサンプル効率を実現し、エージェントが少ない環境との相互作用で学習することを可能にする。

一方で、階層性と世界モデルを組み合わせた研究は少ない。
この２つの要素を組み合わせた数少ないモデルとして、Director\cite{1}やHieros\cite{2}があり、ベースラインとして使用されている世界モデルを上回る性能を示している。
一方で、解釈性やモデルをスケールさせる方向での研究はされておらず、階層的な世界モデルの理論および評価実験は知られていない。

そこで本研究では、階層的世界モデルの現状を理解し課題を特定するために、Hierosの性能評価および内部状態の理解を試みた。
\\
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\

\section{関連研究}
\subsection{世界モデル}
世界モデルは、「説明・数式」。
世界モデルは大きく、1. 状態空間モデル、2. 状態予測モデル、3. 観測予測モデル に分けることができる。
代表的な世界モデルとして、Dreamer および TD-MPC2が挙げられる。
Dreamerは、状態空間モデルの一つであり、「説明」。

\subsection{階層的強化学習 (HRL)}



\subsection{階層的世界モデル (HWM)}
階層的世界モデルも、世界モデルと同様、
状態空間モデルであるDreamerの方策を階層化したものがDirectorである。マネージャーとワーカーの２つの方策が存在し、マネージャーはより長い時間軸で目標とする状態を圧縮された形でワーカーに渡し、ワーカーは通常の強化学習と同様の短い時間軸で行動を出力する。
ワーカーの損失関数にマネージャーの提示した目標がどの程度達成されたかを組み込むことで、ワーカーとマネージャーの２つの方策がそれぞれ役割分担しながら同時に学習できるようにしている。

Directorの拡張としてHierosがある。
Hierosでは、
\begin{enumerate}
   \item 方策だけでなく世界モデルも階層化している。
   \item 2階層だけでなくそれより多い階層でも実験を行っている。
   \item 計算効率を上げるために、状態空間モデルとしてRSSMではなくS5WMを用いている。
   \item リプレイバッファからのサンプリングを工夫し、偏りを減らすことと時間計算量の削減を実現している。
\end{enumerate}
という工夫が加えられたモデルが提案されており、AtariにおいてDreamerV3を超える性能を示した。
本稿では方策だけでなく世界モデルも階層化しているという点でHierosに着目し、その限界と可能性を調べる。


THICKも、Directorをベースとして提案された手法である。「説明」。


状態予測モデルであるTD-MPC2を階層的にしたものがPuppeteerである。これは、「説明」


1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\

\section{実験・考察}
本稿では、階層的世界モデルのHieros\cite{2}モデルの評価を行った。
特に、論文中では検証されていなかった、長期タスクの評価に適しているVisual Pinpad環境での検証を行った。また、Atariでのモデルの内部状態の確認を行った。

\subsection{手法}
Hierosのモデル検証では、既存のレポジトリ\cite{11}を変更して実験を行った。
また、Directorではこのレポジトリ\cite{12}を用いて実験を行った。
RTX-5070TiまたはRTX-6000一つを用いて学習をした。

PinpadにおけるHierosの学習状況を調査した。まずは、Hierosのベースラインでの検証を行った。
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/Hieros-baseline.png}
  \caption{Visual Pinpad環境におけるHierosの学習および内部報酬曲線。}
  \label{fig:hieros-baseline}
\end{figure}

図からわかるように、報酬が0のまま学習が進まないことが確認された。
そこで、以下ではpinpadの報酬設計を元の実装から、末尾の一致度が上がるとスコアが与えられるように変更した（例えば、赤 $\rightarrow$ 赤緑に変化した場合、+1の報酬、赤緑 $\rightarrow$ 赤緑緑に変化した場合、-2の報酬が与えられる）。

より詳しくHierosの性能について調査するために、Hierosの各レイヤが提示している状態の可視化を行った。元の論文では過去の経験を参考にしたノイズを加えての可視化を行っていたが、モデルの予測のみに焦点を当てるために、ノイズなしでの可視化を行った。
update-everyは8で実験を行った。\subsection{時間的抽象化の調整}

update-everyは、それぞれの階層が、下の階層と比較してどの程度時間的抽象化を行っているかを表している。例えば、update-every=4では、4ステップごとに上の階層が下の階層に与えるサブゴールを更新しているという意味である。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-episode-scores.png}
  \caption{異なるsubactor-update-everyパラメータでの学習曲線の比較。}
  \label{fig:sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-subgoal-temporal.png}
  \caption{学習過程におけるサブゴール可視化の時間的変化。典型的でわかりやすい例として15kステップと300kステップを抽出した。}
  \label{fig:subgoal-temporal}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-heatmap-temporal.png}
  \caption{探索位置のヒートマップの時間的変化。}
  \label{fig:heatmap-temporal}
\end{figure}

図からわかるように、多少の差異がある一方で、最終的に報酬が1になる程度に方策が収束している。
また、提示されているサブゴールも、赤までしか学習されておらず、求めている赤緑青のフルシーケンスは学習されなかった。　

\subsection{報酬割り当て係数の変更}
Hierosでは、各レイヤごとの方策は、external reward, subgoal rewardとintrinsic rewardの3つを用いて学習を行っている。
これらの比率を変更することによって、学習がどう変化するかを確認した。

% --- グラフとヒートマップをコンパクトにまとめる ---
\begin{figure}[t]
  \centering
  % 上段：学習曲線
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{media/pinpad/reward-ratio-sweep/sweep-episode-scores.png}
    \caption{異なる報酬割り当て係数の学習曲線}
  \end{subfigure}
  
  \vspace{0.3cm}
  
  % 下段：ヒートマップ
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{media/pinpad/reward-ratio-sweep/sweep-heatmap-temporal.png}
    \caption{異なる報酬割り当て係数の探索位置のヒートマップ}
  \end{subfigure}
  
  \caption{報酬割り当て係数の変更による学習への影響。400kステップ時の探索の偏りを示している。}
  \label{fig:reward-sweep-analysis}
\end{figure}

限定的な向上が確認でき、報酬割り当て係数を変更することの有効性が示唆される。
図3と同様、フルシーケンスを学習したモデルはなく、赤のみのサブゴールを提案するか、直感的には役に立たないサブゴールを学習するかであった。

\subsection{方策エントロピーの変更}

方策エントロピーを変更したときの結果を確認した。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/entropy-sweep/sweep-episode-scores.png}
  \caption{異なるactor\_entropyパラメータでの学習曲線の比較。}
  \label{fig:entropy-sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\columnwidth]{media/pinpad/entropy-sweep/sweep-heatmap-temporal.png}
  \caption{エントロピー調整実験における探索位置のヒートマップの時間的変化。}
  \label{fig:entropy-heatmap-temporal}
\end{figure}

エントロピーが大きくなるとより収束しにくくなり報酬も高くなったが、顕著に学習が進むことはなかった。
サブゴールの可視化については、図3と同様、フルシーケンスを学習したモデルはなく、赤のみのサブゴールを提案するか、直感的には役に立たないサブゴールを学習するかであった。

\subsection{Pinpadの報酬設計の変更}

pinpad-easy-threeタスクにおいて、報酬設計を変更した実験結果。
以下の7種類の報酬設計を比較した：
\begin{itemize}
  \item \textbf{flat（デフォルト）}: タイルを踏んだときの末尾の一致度の増減が報酬
  \item \textbf{progressive}: タイルを踏んだときの末尾の一致度の増減を指数関数的にしたものが報酬（$2^{\text{tail-match}}$）
  \item \textbf{sequence\_bonus}: flatの報酬に加え、末尾一致度が増加ごとにボーナス報酬。
  \item \textbf{decaying}: 時間経過とともに報酬を減衰
  \item \textbf{sparse}: 完全なシーケンス一致時のみ報酬 (元のpinpad)
  \item \textbf{progressive\_steep}: 急勾配な指数関数的報酬（$3^{\text{tail-match}}$）
  \item \textbf{dense\_guidance}: flatの報酬に加えステップごとの細かい報酬（正しい方向に進んだ場合+0.1、間違った方向に進んだ場合-0.1）
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-design-sweep/sweep-episode-scores.png}
  \caption{Pinpad-easy\_threeタスクにおける異なる報酬設計での学習曲線の比較。}
  \label{fig:reward-design-sweep-scores}
\end{figure}

図3と同様、フルシーケンスを学習したモデルはなく、赤のみのサブゴールを提案するか、直感的には役に立たないサブゴールを学習するかであった。
なお、ここには掲載しないが、探索位置のヒートマップからも方策が特定の領域に収束していることが確認された。

\subsection{Atariにおける定性解析}
本節では、AtariベンチマークにおけるHierosの内部挙動を解析する。そのため、学習過程における方策の時間変化を可視化した。
なお計算資源の都合上、freewayのみはHierosの元の論文と同じで(batch size, batch length)が(16, 64)、他は(16, 16)で検証を行った。

% 2段抜きレイアウトで全Atari結果を配置
\begin{figure*}[t]
  \centering
  % --- 1行目：スコア (Score) ---
  % Freeway Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_freeway-scores.png}
    \caption{Freeway: Score}
    \label{fig:freeway-score}
  \end{subfigure}
  \hfill
  % Breakout Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_breakout-scores.png}
    \caption{Breakout: Score}
    \label{fig:breakout-score}
  \end{subfigure}
  \hfill
  % Krull Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_krull-scores.png}
    \caption{Krull: Score}
    \label{fig:krull-score}
  \end{subfigure}
  \hfill
  % Battle Zone Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_battle_zone-scores.png}
    \caption{Battle Zone: Score}
    \label{fig:battlezone-score}
  \end{subfigure}

  \vspace{0.2cm} % 行間の調整

  % --- 2行目：方策 (Policy) ---
  % Freeway Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_freeway-policy-temporal.png}
    \caption{Freeway: Policy}
    \label{fig:freeway-policy}
  \end{subfigure}
  \hfill
  % Breakout Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_breakout-policy-temporal.png}
    \caption{Breakout: Policy}
    \label{fig:breakout-policy}
  \end{subfigure}
  \hfill
  % Krull Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_krull-policy-temporal.png}
    \caption{Krull: Policy}
    \label{fig:krull-policy}
  \end{subfigure}
  \hfill
  % Battle Zone Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_battle_zone-policy-temporal.png}
    \caption{Battle Zone: Policy}
    \label{fig:battlezone-policy}
  \end{subfigure}

  \caption{Atari 100kベンチマークにおける学習結果。上段が学習曲線、下段が400kステップ時点での方策画像（5フレーム間隔で抽出）を示す。Freewayでは単調な前進動作のみが繰り返されていることが確認できる。}
  \label{fig:atari-analysis}
\end{figure*}

図\ref{fig:atari-analysis}に示すように、HierosのAtari環境における学習は、スコアの観点では論文通り高い性能を示しているが、400kステップ時点での高解像度方策画像から抽出した連続フレームを確認すると単純な行動パターンが繰り返されていることが確認される。
特にFreewayでは、スコアは高いものの連続したフレームを確認すると前進動作のみが繰り返されており、複雑な方策は学習できていない。
このことから、Hierosが報告する高スコアは必ずしも「賢い階層的計画」によるものではなく、環境の特性上、単純な方策でも高報酬が得られる可能性が示唆される。
\\
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\

\subsection{考察}
実験当初はpinpadで学習が進まない理由が報酬が疎であるからだと考えていたが、Pinpadの報酬設計を密にした時にも学習が進まなかったため、そうでは無いと考えた。
さらにAtariのモデルの可視化もする中で、Atariでも有意義な方策が学習できていないと思われ、Hierosのモデルそのものに、不安定性あるいは改良の余地があるのではないかと考える。
一方で、有意義だと思えない学習をしているにも関わらず、なぜHierosがAtariで性能が高く出ていたのかの理由の解明も今後の課題である。
Atariの最適方策が学習しやすく、\cite{9}で提示されているような階層化による探索の改善が関連しているとも考えられるが、検証が必要である。
検証結果からはHierosの上手く学習ができておらず汎化性能が低く、より頑健性の高いモデルの理論的、実験的模索が必要であると結論づける。


ベースラインコードにエラーが含まれている可能性は否めない一方で、世界モデルを階層的にすることによって学習が不安定になっている可能性も高いと考える。
特に世界モデルは目的関数や誤差蓄積に起因して、また階層的モデルは階層の同時学習による非定常性によってハイパーパラメータに繊細であることが知られており、それらの組み合わせによって学習が不安定になってしまっている可能性もある。
この理論的、実証実験的解析は今後の課題である。


\section{まとめ}
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\




\section{展望}
本稿ではHierosの3つの報酬係数を手動で設定し比較を行ったが、HarmonyDream \cite{13} のように、報酬を自動でバランスすると、より多様なタスクにおいて学習ができるようになると予想する。
\\
1\\
2\\
3\\
4\\
5\\

\begin{thebibliography}{99}
\bibitem[Director]{1} Hafner, D. et al.: Deep Hierarchical Planning from Pixels, NeurIPS (2022).
\bibitem[Hieros]{2} Mattes, et al.: Hieros - Hierarchical Imagination on Structured State Space Sequence World Models (2023).
\bibitem[DreamerV1]{3} Hafner, D. et al.: Dream to Control: Learning Behaviors by Latent Imagination, ICLR (2020).
\bibitem[DreamerV2]{4} Hafner, D. et al.: Mastering Atari with Discrete World Models, ICLR (2021).
\bibitem[DreamerV3]{5} Hafner, D. et al.: Mastering Diverse Domains through World Models, Nature (2025).
\bibitem[TD-MPC]{6} Hansen, N. et al.: Temporal Difference Learning for Model Predictive Control, ICML (2022).
\bibitem[TD-MPC2]{7} Hansen, N. et al.: TD-MPC2: Scalable, Robust World Models for Continuous Control, ICLR (2024).
\bibitem[Puppeteer]{8} Hansen, N. et al.: Hierarchical World Models as Visual Whole-Body Humanoid Controllers, ICLR (2025).
\bibitem[HRL benefit]{9} Nachum, O. et al.: Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?, NeurIPS (2019).
\bibitem[THICK]{10} Gumbsch, C. et al.: Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics, EWRL (2023).
\bibitem[Hieros Repo]{11} \url{https://github.com/Snagnar/Hieros}
\bibitem[Director Repo]{12} \url{https://github.com/danijar/director}
\bibitem[HarmonyDream]{13} Ma, H. et al.: HarmonyDream: Task Harmonization Inside World Models, ICML (2024).
\bibitem[HRL survey]{14} Pateria, S. et al.: Hierarchical Reinforcement Learning: A Comprehensive Survey, ACM Computing Surveys (2021). 
\end{thebibliography}

\section{Appendix}
\subsection{Director実験結果}
Directorを用いたPinpad-3およびPinpad-easy-3の結果。学習が進んでいることが確認される。pinpad-3の結果は\cite{1}で報告されている結果に近いものとなっている。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/director-results/director-episode-scores.png}
  \caption{DirectorによるPinpad-3およびPinpad-dense-3の学習曲線。}
  \label{fig:director-scores}
\end{figure}

また、比較のために、作成したpinpad-denseでの学習も行い、高い性能が出ることを確認した。これにより、報酬設計には問題がないことが確認された。
なお、GPUのメモリの都合上途中までしか学習ができなかった。

\subsection{RSSM}
HierosはS5WMを用いているが、ベースラインにはRSSMも用意されている。以下は、RSSMを用いてHierosをpinpadで結果。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/rssm-sweep/sweep-episode-scores.png}
  \caption{RSSMを用いたHierosのPinpadでの学習曲線。}
  \label{fig:rssm-scores}
\end{figure}

学習が進まなかったため、max-hierarchy=1のときは320kで学習を打ち切った。
階層数を少ない時により学習が進んでいることから、階層を加えることがモデルの頑健性を下げていることが推測できる。

