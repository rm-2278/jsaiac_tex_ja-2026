%\documentstyle[twocolumn,jsaiac]{jarticle}
%\documentstyle[twocolumn,jsaiac]{j-article}
\documentclass[twocolumn]{jarticle}

\usepackage{jsaiac}

\usepackage{color}
\usepackage[dvipdfmx]{graphicx}
\usepackage{subcaption}
\usepackage{url}
\usepackage{amsmath}
%%
\title{
\jtitle{階層的世界モデルの現状と課題：Hierosの限界と将来の展望}
\etitle{Current Status and Challenges in Hierarchical World Models: Identifying Limitations of Hieros and Future Prospects}
}
\jaddress{三好理輝，rm2278@cam.ac.uk}

\author{%
\jname{三好 理輝\first}
\ename{Riki Miyoshi}
\and
\jname{劉 智優\second}
\ename{Ziwoo You}
\and
\jname{山田 達也\third}
\ename{Yamada Tatsuya}
}

\affiliate{
\jname{\first{}ケンブリッジ大学}
\ename{University of Cambridge}
\and
\jname{\second{}電気通信大学}
\ename{University of Electro-Communications}
\and
\jname{\third{}大阪大学}
\ename{The University of Osaka}
}

%%
\begin{abstract}
   ここに\\
   アブストラクトを\\
   5行程度\\
   書き\\
   ます\\
\end{abstract}

%\setcounter{page}{1}
\def\Style{``jsaiac.sty''}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em%
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\JBibTeX{\leavevmode\lower .6ex\hbox{J}\kern-0.15em\BibTeX}
\def\LaTeXe{\LaTeX\kern.15em2$_{\textstyle\varepsilon}$}

\begin{document}
\maketitle

\section{はじめに}

深層強化学習は多くの分野で成功を収めている一方で，大量の環境相互作用を必要とするサンプル効率の低さや，長期的依存を伴うタスクにおける学習の不安定性が依然として課題である \cite{14,9}。
この課題に対し，環境ダイナミクスを潜在空間上に学習し，内部モデル上で将来状態を予測するワールドモデルが提案されてきた \cite{15,16}。
特に Dreamer 系列 \cite{3,4,5} は，潜在状態空間上での想像学習により高いサンプル効率を実現している。
さらに，長期的な依存関係を扱うために，時間抽象化を導入する階層的強化学習（HRL）が研究されてきた \cite{14,9}。
近年では，世界モデルと階層構造を統合した手法が提案されており，Director \cite{1} や Hieros \cite{2} は，多時間スケールでの計画と想像を同時に実現する枠組みとして報告されている。
これらの手法は，ベースラインの世界モデルを上回る性能を示している点で注目されている。
しかしながら，性能向上が報告されている一方で，階層的世界モデルが内部でどのような抽象表現を獲得しているのか，あるいは真に階層的な想像や長期計画が実現されているのかについての実証的検証は十分ではない。
特に，世界モデル自体を階層化した構造がどのように機能しているのかは明らかになっていない。
そこで本研究では，階層的世界モデルの代表的手法である Hieros \cite{2} に着目し，その性能評価および内部状態の可視化を通じて，階層的想像学習の実態を検証する。
これにより，階層的世界モデルの現状を整理し，構造的課題を明らかにすることを目的とする。

\section{研究背景・目的}

階層的強化学習 (HRL) は，探索において有益であると知られており \cite{9}，効率的な報酬割り当て，転移学習，解釈性などといった点で優位性があると考えられている \cite{14}。
また，世界モデルは高いサンプル効率を実現し，エージェントが少ない環境との相互作用で学習することを可能にする。
一方で，階層性と世界モデルを組み合わせた研究は少ない。
この２つの要素を組み合わせた数少ないモデルとして，Director \cite{1} や Hieros \cite{2} があり，ベースラインとして使用されている世界モデルを上回る性能を示している。
また，解釈性やモデルをスケールさせる方向での研究はされておらず，階層的な世界モデルの理論および評価実験は知られていない。
そこで本研究では，階層的世界モデルの現状を理解し課題を特定するために，Hieros の性能評価および内部状態の理解を試みた。

\section{関連研究}

\subsection{世界モデル}

世界モデルとは，環境の遷移ダイナミクスを学習し，内部モデル上で将来状態を予測することで方策学習を効率化する枠組みである~\cite{15,16}。
強化学習において環境はマルコフ決定過程（MDP）\cite{20} として定式化される：
\begin{equation}
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma)
\end{equation}
ここで，$\mathcal{S}$ は状態空間，$\mathcal{A}$ は行動空間，$P(s_{t+1}\mid s_t, a_t)$ は状態遷移確率，$r(s_t, a_t)$ は報酬関数，$\gamma \in [0,1)$ は将来報酬に対する割引率を表す。
世界モデルは遷移確率 $P$ を近似することで内部モデル上で軌道生成（imagination）を可能にする~\cite{15,3}。
この内部シミュレーションに基づく学習はサンプル効率を大きく向上させることが知られている~\cite{3}。

\subsubsection{世界モデルの分類}

世界モデルは大きく以下の3種類に分類できる：
\begin{itemize}
    \item 状態予測モデル（State Transition Model）
    \item 観測予測モデル（Observation Prediction Model）
    \item 状態空間モデル（State-Space Model）
\end{itemize}
状態予測モデルは現在状態から次状態を直接予測する枠組みであり，観測予測モデルは高次元観測空間を直接予測するが，学習安定性や表現効率の面で課題がある~\cite{15}。
これに対し状態空間モデルは観測を低次元の潜在空間に写像し，その潜在空間上でダイナミクスを学習する手法である~\cite{16}。
近年の世界モデル研究の主流はこの状態空間モデルである。

\subsubsection{Dreamer}

Dreamer 系列~\cite{3,4,5} は潜在状態空間において環境ダイナミクスを学習する代表的な状態空間モデルである。
Recurrent State-Space Model (RSSM) を用いて観測から潜在表現を構築し，その潜在空間上で将来状態および報酬を予測する。
最大の特徴は，実環境ではなく学習済み世界モデル内部で将来軌道を生成し，imagination に基づいて Actor-Critic を最適化する点にある。
DreamerV2 では離散潜在表現が導入され~\cite{4}，DreamerV3 ではスケール適応性および安定性が向上している~\cite{5}。

\subsubsection{TD-MPC2}

TD-MPC2~\cite{7} は潜在世界モデルとモデル予測制御（MPC）を統合した連続制御向け手法であり，TD-MPC~\cite{6} の拡張として提案された。
Dreamer が imagination に基づく方策学習を主軸とするのに対し，TD-MPC2 は学習された潜在モデルを用いてオンラインで行動系列を計画する点に特徴がある。
モデル学習と時間差分学習を統合することで，連続制御タスクにおいて高い精度と安定性を示している~\cite{7}。



\section{実験・考察}
本稿では、階層的世界モデルのHieros\cite{2}モデルの評価を行った。
特に、論文中では検証されていなかった、長期タスクの評価に適しているVisual Pinpad環境での検証を行った。また、Atariでのモデルの内部状態の確認を行った。

\subsection{手法}
Hierosのモデル検証では、既存のレポジトリ\cite{11}を変更して実験を行った。
また、Directorではこのレポジトリ\cite{12}を用いて実験を行った。
RTX-5070TiまたはRTX-6000一つを用いて学習をした。

PinpadにおけるHierosの学習状況を調査した。まずは、Hierosのベースラインでの検証を行った。
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/Hieros-baseline.png}
  \caption{Visual Pinpad環境におけるHierosの学習および内部報酬曲線。}
  \label{fig:hieros-baseline}
\end{figure}

図からわかるように、報酬が0のまま学習が進まないことが確認された。
そこで、以下ではpinpadの報酬設計を元の実装から、末尾の一致度が上がるとスコアが与えられるように変更した（例えば、赤 $\rightarrow$ 赤緑に変化した場合、+1の報酬、赤緑 $\rightarrow$ 赤緑緑に変化した場合、-2の報酬が与えられる）。

より詳しくHierosの性能について調査するために、Hierosの各レイヤが提示している状態の可視化を行った。元の論文では過去の経験を参考にしたノイズを加えての可視化を行っていたが、モデルの予測のみに焦点を当てるために、ノイズなしでの可視化を行った。
update-everyは8で実験を行った。\subsection{時間的抽象化の調整}

update-everyは、それぞれの階層が、下の階層と比較してどの程度時間的抽象化を行っているかを表している。例えば、update-every=4では、4ステップごとに上の階層が下の階層に与えるサブゴールを更新しているという意味である。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-episode-scores.png}
  \caption{異なるsubactor-update-everyパラメータでの学習曲線の比較。}
  \label{fig:sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-subgoal-temporal.png}
  \caption{学習過程におけるサブゴール可視化の時間的変化。典型的でわかりやすい例として15kステップと300kステップを抽出した。}
  \label{fig:subgoal-temporal}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/subactor-update-sweep/sweep-heatmap-temporal.png}
  \caption{探索位置のヒートマップの時間的変化。}
  \label{fig:heatmap-temporal}
\end{figure}

図からわかるように、多少の差異がある一方で、最終的に報酬が1になる程度に方策が収束している。
また、提示されているサブゴールも、赤までしか学習されておらず、求めている赤緑青のフルシーケンスは学習されなかった。　

\subsection{報酬割り当て係数の変更}
Hierosでは、各レイヤごとの方策は、external reward, subgoal rewardとintrinsic rewardの3つを用いて学習を行っている。
これらの比率を変更することによって、学習がどう変化するかを確認した。

% --- グラフとヒートマップをコンパクトにまとめる ---
\begin{figure}[t]
  \centering
  % 上段：学習曲線
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{media/pinpad/reward-ratio-sweep/sweep-episode-scores.png}
    \caption{異なる報酬割り当て係数の学習曲線}
  \end{subfigure}
  
  \vspace{0.3cm}
  
  % 下段：ヒートマップ
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{media/pinpad/reward-ratio-sweep/sweep-heatmap-temporal.png}
    \caption{異なる報酬割り当て係数の探索位置のヒートマップ}
  \end{subfigure}
  
  \caption{報酬割り当て係数の変更による学習への影響。400kステップ時の探索の偏りを示している。}
  \label{fig:reward-sweep-analysis}
\end{figure}

限定的な向上が確認でき、報酬割り当て係数を変更することの有効性が示唆される。
図3と同様、フルシーケンスを学習したモデルはなく、赤のみのサブゴールを提案するか、直感的には役に立たないサブゴールを学習するかであった。

\subsection{方策エントロピーの変更}

方策エントロピーを変更したときの結果を確認した。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/entropy-sweep/sweep-episode-scores.png}
  \caption{異なるactor\_entropyパラメータでの学習曲線の比較。}
  \label{fig:entropy-sweep-scores}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\columnwidth]{media/pinpad/entropy-sweep/sweep-heatmap-temporal.png}
  \caption{エントロピー調整実験における探索位置のヒートマップの時間的変化。}
  \label{fig:entropy-heatmap-temporal}
\end{figure}

エントロピーが大きくなるとより収束しにくくなり報酬も高くなったが、顕著に学習が進むことはなかった。
サブゴールの可視化については、図3と同様、フルシーケンスを学習したモデルはなく、赤のみのサブゴールを提案するか、直感的には役に立たないサブゴールを学習するかであった。

\subsection{Pinpadの報酬設計の変更}

pinpad-easy-threeタスクにおいて、報酬設計を変更した実験結果。
以下の7種類の報酬設計を比較した：
\begin{itemize}
  \item \textbf{flat（デフォルト）}: タイルを踏んだときの末尾の一致度の増減が報酬
  \item \textbf{progressive}: タイルを踏んだときの末尾の一致度の増減を指数関数的にしたものが報酬（$2^{\text{tail-match}}$）
  \item \textbf{sequence\_bonus}: flatの報酬に加え、末尾一致度が増加ごとにボーナス報酬。
  \item \textbf{decaying}: 時間経過とともに報酬を減衰
  \item \textbf{sparse}: 完全なシーケンス一致時のみ報酬 (元のpinpad)
  \item \textbf{progressive\_steep}: 急勾配な指数関数的報酬（$3^{\text{tail-match}}$）
  \item \textbf{dense\_guidance}: flatの報酬に加えステップごとの細かい報酬（正しい方向に進んだ場合+0.1、間違った方向に進んだ場合-0.1）
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/reward-design-sweep/sweep-episode-scores.png}
  \caption{Pinpad-easy\_threeタスクにおける異なる報酬設計での学習曲線の比較。}
  \label{fig:reward-design-sweep-scores}
\end{figure}

図3と同様、フルシーケンスを学習したモデルはなく、赤のみのサブゴールを提案するか、直感的には役に立たないサブゴールを学習するかであった。
なお、ここには掲載しないが、探索位置のヒートマップからも方策が特定の領域に収束していることが確認された。

\subsection{Atariにおける定性解析}
本節では、AtariベンチマークにおけるHierosの内部挙動を解析する。そのため、学習過程における方策の時間変化を可視化した。
なお計算資源の都合上、freewayのみはHierosの元の論文と同じで(batch size, batch length)が(16, 64)、他は(16, 16)で検証を行った。

% 2段抜きレイアウトで全Atari結果を配置
\begin{figure*}[t]
  \centering
  % --- 1行目：スコア (Score) ---
  % Freeway Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_freeway-scores.png}
    \caption{Freeway: Score}
    \label{fig:freeway-score}
  \end{subfigure}
  \hfill
  % Breakout Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_breakout-scores.png}
    \caption{Breakout: Score}
    \label{fig:breakout-score}
  \end{subfigure}
  \hfill
  % Krull Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_krull-scores.png}
    \caption{Krull: Score}
    \label{fig:krull-score}
  \end{subfigure}
  \hfill
  % Battle Zone Score
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_battle_zone-scores.png}
    \caption{Battle Zone: Score}
    \label{fig:battlezone-score}
  \end{subfigure}

  \vspace{0.2cm} % 行間の調整

  % --- 2行目：方策 (Policy) ---
  % Freeway Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_freeway-policy-temporal.png}
    \caption{Freeway: Policy}
    \label{fig:freeway-policy}
  \end{subfigure}
  \hfill
  % Breakout Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_breakout-policy-temporal.png}
    \caption{Breakout: Policy}
    \label{fig:breakout-policy}
  \end{subfigure}
  \hfill
  % Krull Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_krull-policy-temporal.png}
    \caption{Krull: Policy}
    \label{fig:krull-policy}
  \end{subfigure}
  \hfill
  % Battle Zone Policy
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/atari/atari_battle_zone-policy-temporal.png}
    \caption{Battle Zone: Policy}
    \label{fig:battlezone-policy}
  \end{subfigure}

  \caption{Atari 100kベンチマークにおける学習結果。上段が学習曲線、下段が400kステップ時点での方策画像（5フレーム間隔で抽出）を示す。Freewayでは単調な前進動作のみが繰り返されていることが確認できる。}
  \label{fig:atari-analysis}
\end{figure*}

図\ref{fig:atari-analysis}に示すように、HierosのAtari環境における学習は、スコアの観点では論文通り高い性能を示しているが、400kステップ時点での高解像度方策画像から抽出した連続フレームを確認すると単純な行動パターンが繰り返されていることが確認される。
特にFreewayでは、スコアは高いものの連続したフレームを確認すると前進動作のみが繰り返されており、複雑な方策は学習できていない。
このことから、Hierosが報告する高スコアは必ずしも「賢い階層的計画」によるものではなく、環境の特性上、単純な方策でも高報酬が得られる可能性が示唆される。
\\
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\

\subsection{考察}
実験当初はpinpadで学習が進まない理由が報酬が疎であるからだと考えていたが、Pinpadの報酬設計を密にした時にも学習が進まなかったため、そうでは無いと考えた。
さらにAtariのモデルの可視化もする中で、Atariでも有意義な方策が学習できていないと思われ、Hierosのモデルそのものに、不安定性あるいは改良の余地があるのではないかと考える。
一方で、有意義だと思えない学習をしているにも関わらず、なぜHierosがAtariで性能が高く出ていたのかの理由の解明も今後の課題である。
Atariの最適方策が学習しやすく、\cite{9}で提示されているような階層化による探索の改善が関連しているとも考えられるが、検証が必要である。
検証結果からはHierosの上手く学習ができておらず汎化性能が低く、より頑健性の高いモデルの理論的、実験的模索が必要であると結論づける。


ベースラインコードにエラーが含まれている可能性は否めない一方で、世界モデルを階層的にすることによって学習が不安定になっている可能性も高いと考える。
特に世界モデルは目的関数や誤差蓄積に起因して、また階層的モデルは階層の同時学習による非定常性によってハイパーパラメータに繊細であることが知られており、それらの組み合わせによって学習が不安定になってしまっている可能性もある。
この理論的、実証実験的解析は今後の課題である。


\section{まとめ}
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
9\\
10\\




\section{展望}
本稿ではHierosの3つの報酬係数を手動で設定し比較を行ったが、HarmonyDream \cite{13} のように、報酬を自動でバランスすると、より多様なタスクにおいて学習ができるようになると予想する。
\\
1\\
2\\
3\\
4\\
5\\

\begin{thebibliography}{99}
\bibitem[Director]{1} Hafner, D. et al.: Deep Hierarchical Planning from Pixels, NeurIPS (2022).
\bibitem[Hieros]{2} Mattes, et al.: Hieros - Hierarchical Imagination on Structured State Space Sequence World Models (2023).
\bibitem[DreamerV1]{3} Hafner, D. et al.: Dream to Control: Learning Behaviors by Latent Imagination, ICLR (2020).
\bibitem[DreamerV2]{4} Hafner, D. et al.: Mastering Atari with Discrete World Models, ICLR (2021).
\bibitem[DreamerV3]{5} Hafner, D. et al.: Mastering Diverse Domains through World Models, Nature (2025).
\bibitem[TD-MPC]{6} Hansen, N. et al.: Temporal Difference Learning for Model Predictive Control, ICML (2022).
\bibitem[TD-MPC2]{7} Hansen, N. et al.: TD-MPC2: Scalable, Robust World Models for Continuous Control, ICLR (2024).
\bibitem[Puppeteer]{8} Hansen, N. et al.: Hierarchical World Models as Visual Whole-Body Humanoid Controllers, ICLR (2025).
\bibitem[HRL benefit]{9} Nachum, O. et al.: Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?, NeurIPS (2019).
\bibitem[THICK]{10} Gumbsch, C. et al.: Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics, EWRL (2023).
\bibitem[Hieros Repo]{11} \url{https://github.com/Snagnar/Hieros}
\bibitem[Director Repo]{12} \url{https://github.com/danijar/director}
\bibitem[HarmonyDream]{13} Ma, H. et al.: HarmonyDream: Task Harmonization Inside World Models, ICML (2024).
\bibitem[HRL survey]{14} Pateria, S. et al.: Hierarchical Reinforcement Learning: A Comprehensive Survey, ACM Computing Surveys (2021). 
\bibitem[WorldModels]{15} Ha, D. and Schmidhuber, J.: World Models, NeurIPS (2018).
% 追加分
\bibitem[PlaNet]{16} Hafner, D. et al.: Learning Latent Dynamics for Planning from Pixels, ICML (2019).
\bibitem[SuttonBarto]{20}Sutton, R. S. and Barto, A. G.: Reinforcement Learning: An Introduction, MIT Press (2018).


\end{thebibliography}

\section{Appendix}
\subsection{Director実験結果}
Directorを用いたPinpad-3およびPinpad-easy-3の結果。学習が進んでいることが確認される。pinpad-3の結果は\cite{1}で報告されている結果に近いものとなっている。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/director-results/director-episode-scores.png}
  \caption{DirectorによるPinpad-3およびPinpad-dense-3の学習曲線。}
  \label{fig:director-scores}
\end{figure}

また、比較のために、作成したpinpad-denseでの学習も行い、高い性能が出ることを確認した。これにより、報酬設計には問題がないことが確認された。
なお、GPUのメモリの都合上途中までしか学習ができなかった。

\subsection{RSSM}
HierosはS5WMを用いているが、ベースラインにはRSSMも用意されている。以下は、RSSMを用いてHierosをpinpadで結果。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{media/pinpad/rssm-sweep/sweep-episode-scores.png}
  \caption{RSSMを用いたHierosのPinpadでの学習曲線。}
  \label{fig:rssm-scores}
\end{figure}

学習が進まなかったため、max-hierarchy=1のときは320kで学習を打ち切った。
階層数を少ない時により学習が進んでいることから、階層を加えることがモデルの頑健性を下げていることが推測できる。



\end{document}
